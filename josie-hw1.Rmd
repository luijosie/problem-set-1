---
title: HW1
author: Wai Laam "Josie" Lui
date: October 24, 2019
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r preamb, message=FALSE, warning=FALSE}
setwd("~/iCloud/CourseWork/josie-19-aut/problem-set-1")
library(tidyverse)
library(skimr)
library(dendextend)
library(seriation) # for diss plot
# additional libraries for GMM
library(mixtools)
library(plotGMM)
library(knitr) # kable
library(scales) # for color
library(clValid) # for internal validation
library(mclust) # for clValid
#library(Rmisc) # will call namespace directly when needed
rm(list=ls()) # clean working environment
```
# Loading the Data
```{r p1}
load("State Leg Prof Data & Codebook/legprof-components.v1.0.RData")
```

# Cleaning the Data
```{r p2}
df2 <- x %>% filter(sessid=="2009/10") %>%
  select(state,expend,salary_real,t_slength,slength) %>%
  drop_na(expend,salary_real,t_slength,slength) # one is dropped. 49 are left.
state <- df2 %>% pull(state) %>% factor(levels=state.name,labels=state.name)
df2 <- df2 %>% select(-state) %>% scale() %>% as.tibble() # clean DF
```

# Determining Clusterability

## Informal Approach
```{r p3a}
# visual inspection
g1 <- ggplot(df2) +
  geom_point(aes(x=salary_real,y=t_slength))+
  labs(x="Real Salary",y="Total Session Length")
g2 <- ggplot(df2)+
  geom_point(aes(x=salary_real,y=slength))+
  labs(x="Real Salary",y="Session Length")
g3 <- ggplot(df2)+
  geom_point(aes(x=expend,y=t_slength))+
  labs(x="Expenditure",y="Total Session Length")
g4 <- ggplot(df2)+
  geom_point(aes(x=expend,y=slength))+
  labs(x="Expenditure",y="Session Length")
Rmisc::multiplot(g1, g2, g3, g4, cols=2)
```

I can see roughly two bocks in the plots in the left column where "Real Salary" is ploted on the x-axis. I see some outliers in the two plots with "Expenditure" on the x-axis, but I don't see multiple clusters at least in 2-D. However, the distribution of points is far from random/natural as well.

## VAT Approach
```{r p3b}
df2_dist <- dist(df2) 
dissplot(df2_dist)
```

The VAT shows roughly two clusters - a big one in the bottom right and a much smaller one in the top left. Within the big cluster, there also appears to be smaller sub-clusters.

## Hopkins Test
The data size is not so big (49), so we will choose 20 as the sampling size.

```{r p3c}
hop <- function(){ # function to perform the Hopkins test once
  return(clustertend::hopkins(df2,n=10)$H)
}
set.seed(97323)
hopstat = replicate(100,hop()) # we will run the hopkins test 100 times to avoid judgment based on one random test
hist(hopstat,main="Hopkins Test Distribution for 09-10 State Legislature",
     xlab="Hopkins Statistic",
     ylab="Counts (total = 100)")
```

From the plot we see that the central/majority tendency is around 0.15-0.20, which is much below the 0.5 threshold. We have reason to claim clusterbility.

## Summary

Based on informal scatter plots, the ODI plot, and the empirical distribution of the Hopkins statistic, we have strong reason to suspect the presence of clusters. We may proceed with the various clustering methods.

# Simple Agglomerative Hiearchical Cluster
```{r p4, fig.width=6,fig.height=10}
hc_complete <- hclust(df2_dist, #use the distance matrix
                      method = "complete");
hc_plot <- as.dendrogram(hc_complete) # to allow horizontal dendrograms
labels(hc_plot) <- state.abb[labels(hc_plot)] # assign labels in correct order
plot(hc_plot,horiz = TRUE) # dendrogram
cuts <- cutree(hc_complete, 
               k = c(3,4))
table(`3 Clusters` = cuts[,1], 
      `4 Clusters` = cuts[,2])
```

We will use the "complete" linkage method to capture a balanced overall grouping. In the analysis below, the letter n refers to the number of clusters. 

Moving down from the top level (n=2), we first notice a bifurcation between CA, MA, NY, PA, OH, IL, and MI (the minority block) and all the other states (the majority block). The minority states tend do be Democratic states with either strong economic output or workers' presence.

Moving one level down (n=3), we see CA breaking off the "minority block" and becomes the third cluster all by itself.

Moving further down (n=4), we see MA and NY - highly-developed New England states - break off with PA, OH, IL, and MI - the majority being mid-western states whose economy is more driven by the manufacturing and agricultural sectors.

At height < 4, we finally see the bigger block branch off (n=5). To be honest, I don't have any insights to this split. However, since this split occured at a lower level then NY and MA branching off, it is fair to say that the overall distance within the majority block is smaller than the distance between the two New England states and the Midwestern states.


# K-means at n=2
```{r p5}
set.seed(1289)
kmeans <- kmeans(df2, 
                 centers = 2,
                 nstart = 100)
# the following code snippet is adapted from my personal submission 
# to an assignment in Computational Social Science
# Needed for USA polygons
# devtools::install_github("hrbrmstr/albersusa")
library(albersusa)
# Albers equal-area conic convenience projection
state_maps <- usa_sf("laea") %>% 
  filter(name != "District of Columbia") %>%
  mutate(state = as.character(name) %>% factor(levels=state.name,labels=state.name))
kmeans_group <- tibble(state,group = factor(kmeans$cluster))
map_cluster <- kmeans_group %>% full_join(state_maps,by="state")
ggplot()+
  geom_sf(data = map_cluster,
          aes(geometry=geometry,
              fill = as.factor(group)),
          lwd = 0.3)+
  scale_fill_manual(values = c("#d01c8b", "#4d9220"),na.value = "darkgray", name = "Cluster Group")+
  labs(title = "K-means Cluster Map for n=2",
       subtitle = "Wisconsin (NA) included for better geographical reference.")
kable(kmeans_group %>% filter(group==2))
```

The minority block according to K-means (n=2) is almost the same as the results obtained via the Dendrogram. Only deviation is that IL now belongs to the majority group.

# Gaussian Mixture Model
```{r p6} 
set.seed(3919) 
gmm1 <- mvnormalmixEM(df2, 
                    k = 2)
post <- gmm1$posterior
gmm_group <- tibble(state, 
                    group = factor(2-as.numeric(post[,1]>post[,2])),
                    comp1 = post[,1]) # true = 1; false =2
kable(gmm_group %>% filter(group==2))
```

Compared to the K-means output, six new states joined the smaller cluster. They are: AK, AZ, FL, NV, NJ, TX. Again, IL is not part of the smaller cluster. 

```{r p6map, fig.width = 12,fig.height=6}
map_cluster <-gmm_group %>% full_join(state_maps,by="state")
g1 <- ggplot()+
  geom_sf(data = map_cluster,
          aes(geometry=geometry,
              fill = as.factor(group)),
          lwd = 0.1)+
  scale_fill_manual(values = c("#d01c8b", "#4d9220"),na.value = "darkgray", name = "Cluster Group")+
  labs(title = "GMM Cluster for n=2 - Binary Map",
       subtitle = "Wisconsin (NA) included for better geographical reference.")+
  theme(legend.position="bottom")
g2 <- ggplot()+
  geom_sf(data = map_cluster,
          aes(geometry=geometry,
              fill = comp1),
          lwd = 0.3)+
  scale_fill_distiller(type="div",palette="PiYG",na.value = "darkgray",name = "Likelihood in Group 1")+
  labs(title = "GMM Cluster for n=2 - Probability Map",
       subtitle = "Wisconsin (NA) included for better geographical reference.")+
  theme(legend.position="bottom")
Rmisc::multiplot(g1, g2,cols=2)
```

Comparing the binary map to the probablistic map, we see that the coefficients for component 1 versus component 2 in the GMM are quite dichotomous - all but few states stand at the two extremes and the states with lighter shades such as CA, WA, and CO aren't too far away from the extremities either. Overall, the GMM gave a quite clear clustering result.

# Plot and Discuss
Let us first look back to the agglomerative model and extract the cluster assignment at k=2.
```{r p70}
cut <- cutree(hc_complete, k=2)
ahc_group <- tibble(state,group = factor(cut))
```
```{r p71, fig.width = 12,fig.height=4}
# display k-means & GMM cluster through coloring for total session length vs. real salary
g1 <- ggplot(data = bind_cols(df2,ahc_group)) +
  geom_point(aes(x=salary_real,y=t_slength,color=group))+
  labs(title = "Agglomerative Hiearchical Model")
g2 <- ggplot(data = bind_cols(df2,kmeans_group)) +
  geom_point(aes(x=salary_real,y=t_slength,color=group))+
  labs(title = "K-Means")
g3 <- ggplot(data = bind_cols(df2,gmm_group)) +
  geom_point(aes(x=salary_real,y=t_slength,color=group))+
  labs(title = "GMM(2)")
Rmisc::multiplot(g1, g2, g3,cols=3)
```
In these two dimensions, IL - approximately (1.6,0.2) in plot 1 - is indeed sort of marginal with respect to the two major groups. As a result, it is perhaps reasonable that only the agglomerative mixture model assigned it to the minority block, while both the K-means clustering and GMM(2) clustering captured the six states that obviously stood out from our informal scatter plot observations in problem 1. Compared to k-means, the GMM(2) method captured six additional states into the smaller cluster, but looking at total session length vs. real salary we don't quite understand why those six states stood out. We will attempt to see why in the plots below.

```{r p72, fig.width = 12,fig.height=4}
# display k-means & GMM cluster through coloring for total session length vs. real salary
g1 <- ggplot(data = bind_cols(df2,ahc_group)) +
  geom_point(aes(x=salary_real,y=expend,color=group))+
  labs(title = "Agglomerative Hiearchical Model")
g2 <- ggplot(data = bind_cols(df2,kmeans_group)) +
  geom_point(aes(x=salary_real,y=expend,color=group))+
  labs(title = "K-Means")
g3 <- ggplot(data = bind_cols(df2,gmm_group)) +
  geom_point(aes(x=salary_real,y=expend,color=group))+
  labs(title = "GMM(2)")
Rmisc::multiplot(g1, g2, g3, cols=3)
```

We now see many more outliers in the colored plot for GMM(2). Those six additional states likely stood out due to having larger staff expenditure relative to their statesmen salary. However, IL does appear to be in the outlier group when looking at these two dimensions (expenditure vs. salary), yet neither the k-means model nor the gaussian mixture model was able to capture this deviation by assigning IL to the smaller group.


# Validatiion of Methods
```{r p8, message=FALSE, warning=FALSE}
internal_all <- clValid(data.frame(df2), 2:6, 
                    clMethods = c("hierarchical", "kmeans", "model"), metric = "euclidean",
                    validation = "internal",method="complete"); summary(internal_all)
```

We will use the silhouette statistic to evaluate the different clustering methods and configurations. A silhouette score as high as possible within the range [-1,1] indicates strong intra-cluster similarity and strong inter-cluster divergence. For k=2, by the silhouette measure, k-means has the best performance (closest to 1), and hierarchical model has the worst performance. The GMM performs in between We will examine the internal validation summary in the next problem.

# Discussion of Validation

## Takeaways

According to the silhouette measure, k=2 is by far the best cluster number for all three methods. Further splits of existing clusters (in the case for hierarchical model) or a defined addition of cluster numbers will likely result in lower inter-cluster divergence at a larger trade=off compared to the gains from intra-cluster similarity, thus lowering the silhouette statistic. However, it's also worthy to mention that by the silhouette statistic, all three models perform quite similarly, since the statistics range between 0.62 and 0.65, which could be considered comparable without the presence of a hypothesis test.

## Optimal Clustering

As mentioned previously, k=2 is the best cluster number conditioned on each clustering method. When k is set to 2 for all methods, the k-means method performs slightly better than the other two.

## Overriding Validation Results

I can think of three major reason driving towards a technically "sub-optimal" decision. The first reason is that sometimes researchers might have to choose a soft-clustering method over hard-clustering method. Both the hierarchical model and the k-means model incur hard-partitioning, and only GMM allows for subtlety and mixture of cluster assignment based on likelihood. In many cases, topics in the social sciences involve many grayish or even ambiguous topics. In such event, hard clustering might not be the best option, since it may force us to ignore some signal or intricate differences. By examining GMM component scores, a scholar can gain broader insights into the interplay among multiple clusters. Should they like, they can further interpret the clustering in different ways by adjusting the cutoff thresholds to obtain an output clustering assignment more consistent with their claims. However, I am not sure this is a sound practice.

Secondly, an optimal validation score might be considered over-fitting/over-partioning when evaluated based on subject-matter knowledge. The researcher might also want to limit the number of clusters if they feel the optimal algorithm is in fact over-performing. Again, similar to the first reason, they would override validation recommendations to better adjust to their theories.

Thirdly, which in some ways could be considered trivial, there might not be a uniformly "optimal" configuration across different validation metrics. In this case the problem beccomes moot and the human expert has to make a decision.
